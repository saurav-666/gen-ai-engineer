{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Chat Completions - Solved\n",
    "\n",
    "For more information, see [MS Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python-new)\n",
    "\n",
    "The Azure Key and Endpoint are set in your Codespace's Environment automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../../')\n",
    "import init_creds as creds\n",
    " \n",
    "AZURE_OPENAI_KEY = creds.get_api_key()\n",
    "AZURE_OPENAI_ENDPOINT = creds.get_endpoint()\n",
    " \n",
    "if not AZURE_OPENAI_KEY:\n",
    "    raise ValueError(\"No AZURE_OPENAI_KEY set for Azure OpenAI API\")\n",
    "if not AZURE_OPENAI_ENDPOINT:\n",
    "    raise ValueError(\"No AZURE_OPENAI_ENDPOINT set for Azure OpenAI API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BALOvBHlPnRjyoffGGr1fpdvOwHQZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of New York is Albany.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1741805245, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_b705f0c291', usage=CompletionUsage(completion_tokens=8, prompt_tokens=38, total_tokens=46, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "The capital of New York is Albany.\n"
     ]
    }
   ],
   "source": [
    "# Sample Chat Completion using Azure OpenAI API\n",
    "from openai import AzureOpenAI\n",
    "from openai import OpenAIError\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=\"2024-07-01-preview\"\n",
    ")\n",
    "\n",
    "prePrompt = \"You are a helpful assistant that knows about US geography, topography, flora, and fauna.\"\n",
    "prompt = \"\"\"What is the capital of New York?\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prePrompt},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "try:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    print(chat_completion)\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Refactor the code above so that the prePrompt and Prompt can be supplied as parameters to a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor the code above so that the prePrompt and Prompt can be supplied as parameters to a function\n",
    "def chat_completion(prePrompt, prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prePrompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: An error occurred:\\n{e}\"\n",
    "    \n",
    "prePrompt = \"You are a helpful assistant that knows about US geography, topography, flora, and fauna.\"\n",
    "prompt = \"\"\"What is the capital of New York?\"\"\"\n",
    "print(chat_completion(prePrompt, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Add error handling to the function to catch the openai.error.OpenAIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add error handling to the function to catch the openai.error.OpenAIError\n",
    "def chat_completion(prePrompt, prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prePrompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except OpenAIError as e:\n",
    "        return f\"Error: An error occurred in the OpenAI API:\\n{e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An error occurred:\\n{e}\"\n",
    "    \n",
    "prePrompt = \"You are a helpful assistant that knows about US geography, topography, flora, and fauna.\"\n",
    "prompt = \"\"\"What is the capital of New York?\"\"\"\n",
    "print(chat_completion(prePrompt, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: Add the **temperature** and **top_p**, also experiment with the **max_tokens**, **n**, and **stop** parameters. When might one want to use the latter three?\n",
    "- **temperature**: Controls the randomness of the output. Lower values make the output more deterministic, while higher values make it more random.\n",
    "- **top_p**: Controls diversity via nucleus sampling. 0.9 means only the top 90% of probability mass is considered.\n",
    "- **max_tokens**: The maximum number of tokens to generate in the completion.\n",
    "- **n**: The number of completions to generate\n",
    "- **stop**: Define stop sequences if needed\n",
    "\n",
    "Try this prompt to see the parameters in action:\n",
    "\n",
    "```Write a short story about a robot learning to understand human emotions.```\n",
    "\n",
    "How do the various parameters affect your output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the **temperature** and **top_p**, also experiment with the **max_tokens**, **n**, and **stop** parameters to the function.\n",
    "def chat_completion(prePrompt, prompt, temperature=0.7, top_p=1, max_tokens=150, n=1, stop=None):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prePrompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            n=n,\n",
    "            stop=stop\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except OpenAIError as e:\n",
    "        return f\"Error: An error occurred in the OpenAI API:\\n{e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An error occurred:\\n{e}\"\n",
    "    \n",
    "prePrompt = \"You are a helpful assistant that knows how to tell a good story, spin a great yarn, and write a tall tale.\"\n",
    "prompt = \"\"\"Write a short story about a robot learning to understand human emotions.\"\"\"\n",
    "print(\"temperature=0.7, top_p=1, max_tokens=500, n=1, stop=None\")\n",
    "print(chat_completion(prePrompt, prompt, temperature=0.7, top_p=1, max_tokens=500, n=1, stop=None))\n",
    "print('-----------------------------------')\n",
    "print(\"temperature=0.2, top_p=1, max_tokens=500, n=1, stop=None\")\n",
    "print(chat_completion(prePrompt, prompt, temperature=0.2, top_p=1, max_tokens=500, n=1, stop=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4: Add context to the conversation so that you can continue the dialog instead of each prompt standing on its own.\n",
    "1) Store conversation history\n",
    "2) Append new user inputs to the conversation history.\n",
    "3) Send the entire conversation history as context in each API call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context to the conversation so that you can continue the dialog instead of each prompt standing on its own.\n",
    "# 1) Store conversation history\n",
    "# 2) Append new user inputs to the conversation history.\n",
    "# 3) Send the entire conversation history as context in each API call. \n",
    "\n",
    "def chat_completion(prePrompt, prompt, conversation_history, temperature=0.7, top_p=1, max_tokens=5000, n=1, stop=None):\n",
    "    if conversation_history is None or len(conversation_history) == 0:\n",
    "        conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": prePrompt}\n",
    "        ]\n",
    "    conversation_history.append(\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=conversation_history,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            n=n,\n",
    "            stop=stop\n",
    "        )\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        conversation_history.append(\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        )\n",
    "        return response, conversation_history\n",
    "    except OpenAIError as e:\n",
    "        return f\"Error: An error occurred in the OpenAI API:\\n{e}\", conversation_history\n",
    "    except Exception as e:\n",
    "        return f\"Error: An error occurred:\\n{e}\", conversation_history\n",
    "    \n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "prePrompt = \"You are a helpful assistant that knows about US geography, topography, flora, and fauna.\"\n",
    "prompt = \"\"\"What is the capital of New York?\"\"\"\n",
    "response, conversation_history = chat_completion(prePrompt, prompt, conversation_history)\n",
    "print(response)\n",
    "prompt = \"\"\"How many people live there?\"\"\"\n",
    "response, conversation_history = chat_completion(None, prompt, conversation_history)\n",
    "print(response)\n",
    "prompt = \"\"\"What is the weather like?\"\"\"\n",
    "response, conversation_history = chat_completion(None, prompt, conversation_history)\n",
    "print(response)\n",
    "prompt = \"\"\"What is there to do there?\"\"\"\n",
    "response, conversation_history = chat_completion(None, prompt, conversation_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5: Use TikToken to make sure the conversation (Exercise 4) doesn't exceed the model's token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TikToken to make sure the conversation (Exercise 4) doesn't exceed the model's token limit.\n",
    "# 1) Add a stop parameter to the function that will be used to stop the conversation when the token limit is reached.\n",
    "# 2) Use the stop parameter to stop the conversation when the token limit is reached.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "maximum_tokens = 128000\n",
    "# Load the encoding for the model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def measure_token_length(text):\n",
    "    # Encode the text to get the tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Return the length of the tokens\n",
    "    return len(tokens)\n",
    "\n",
    "def chat_completion(prePrompt, prompt, conversation_history = [], temperature=0.7, top_p=1, max_tokens=150, n=1, stop=None):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prePrompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    try:\n",
    "        text = conversation_history.append(messages)\n",
    "        token_length = measure_token_length(text)\n",
    "        if token_length > maximum_tokens:\n",
    "            return f\"Error: The conversation exceeds the token limit of {maximum_tokens}.\"\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=conversation_history.append(messages),\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            n=n,\n",
    "            stop=stop\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except OpenAIError as e:\n",
    "        return f\"Error: An error occurred in the OpenAI API:\\n{e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An error occurred:\\n{e}\"\n",
    "    \n",
    "def chat_completion(prePrompt, prompt, conversation_history, temperature=0.7, top_p=1, max_tokens=5000, n=1, stop=None):\n",
    "    if conversation_history is None or len(conversation_history) == 0:\n",
    "        conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": prePrompt}\n",
    "        ]\n",
    "    conversation_history.append(\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "    try:\n",
    "        # use a list comprehension to convert the conversation history to a single string\n",
    "        text = \" \".join([message[\"content\"] for message in conversation_history])\n",
    "        token_length = measure_token_length(text)\n",
    "        if token_length > maximum_tokens:\n",
    "            return f\"Error: The conversation exceeds the token limit of {maximum_tokens}.\"\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=conversation_history,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            n=n,\n",
    "            stop=stop\n",
    "        )\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        conversation_history.append(\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        )\n",
    "        return response, conversation_history\n",
    "    except OpenAIError as e:\n",
    "        return f\"Error: An error occurred in the OpenAI API:\\n{e}\", conversation_history\n",
    "    except Exception as e:\n",
    "        return f\"Error: An error occurred:\\n{e}\", conversation_history\n",
    "    \n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "prePrompt = \"You are a helpful assistant that knows about US geography, topography, flora, and fauna.\"\n",
    "prompt = \"\"\"What is the capital of New York?\"\"\"\n",
    "response, conversation_history = chat_completion(prePrompt, prompt, conversation_history)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
